for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
for item in tree.xpath("//li"):
    a = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' alpha ')]/text()")[0]
    b = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' betahaus ')]/text()")[0]
    c = item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' capitalism ')]/text()")[0]
    d =    item.xpath(".//div[contains(concat(' ', normalize-space(@class), ' '), ' doughnuts-of-the-realm ')]/a")[0].attrib['href']
    g = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' dontcare ')]/text()")[0]
    h = item.xpath(".//span[contains(concat(' ', normalize-space(@type), ' '), ' foo ')]/text()")
